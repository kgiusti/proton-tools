#!/usr/bin/env python
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
import optparse, sys, re
import csv, time, os, os.path
from proton_tests.common import MessengerSenderC, MessengerReceiverC, \
    free_tcp_ports


def parse_msgr_output( output ):
    """
    Parses the output from the msgr-send/msgr-receive tools, extracting the
    throughput and average latency values.
    Expects output to be in the following pattern:
        Messages sent: %d recv: %d\n
        Total time: %f sec\n
        Throughput: %f msgs/sec\n
        Latency (sec): %f min %f max %f avg\n
    Extracts the values for throughput and average latency, and returns them as
    floats in a tuple of the form (<average_latency>, <throughput>).
    """
    regexp_float = "([-+]?\d+\.\d*)?"
    regexp_throughput = "^Throughput: %s msgs/sec" % regexp_float
    regexp_latency = "^Latency \(sec\): %s min %s max %s avg" % (regexp_float,
                                                                 regexp_float,
                                                                 regexp_float)
    try:
        l = re.search( regexp_latency, output, re.MULTILINE )
        t = re.search( regexp_throughput, output, re.MULTILINE )
        return (float(l.groups()[2]), float(t.groups()[0]))
    except:
        print( "Unable to parse output from msgr-send/msgr-recv!\n"
               "  Has the format of the output changed???" )
        raise


def run_test( receivers, senders, verbose=False ):
    """
    Run a test using a set of senders and receivers.
    """

    for R in receivers:
        R.start( verbose )

    for S in senders:
        S.start( verbose )

    for S in senders:
        S.wait()
        assert S.status() == 0, "Command '%s' failed" % str(S.cmdline())

    for R in receivers:
        R.wait()
        assert R.status() == 0, "Command '%s' failed" % str(R.cmdline())


def test_loopback_64( iterations, timeout=0, verbose=False ):
    """
    Loopback test - one sender, one receiver, 64 byte messages.
    $ msgr-recv -a amqp://~0.0.0.0:9999 -c 100000 -R -b 2048
    $ msgr-send -a amqp://0.0.0.0:9999 -c 100000 -b 64 -p 1024 -R
    """

    msg_count = 1000000
    port = free_tcp_ports()[0]

    receivers = []
    receiver = MessengerReceiverC()
    receiver.subscriptions = ["amqp://~0.0.0.0:%s" % port]
    receiver.receive_count = msg_count
    receiver.send_reply = True
    receiver.timeout = timeout
    receiver.recv_count = 2048
    receivers.append( receiver )

    senders = []
    sender = MessengerSenderC()
    sender.targets = ["amqp://0.0.0.0:%s" % port]
    sender.send_count = msg_count
    sender.get_reply = True
    sender.send_batch = 1024
    sender.msg_size = 64
    sender.timeout = timeout
    senders.append( sender )

    latencies = []
    throughputs = []

    for i in range(iterations):
        run_test( receivers, senders, verbose )
        # results from sender should be "good enough"
        results = parse_msgr_output( sender.stdout() )
        latencies.append( results[0] )
        throughputs.append( results[1] )

    return (latencies, throughputs)


def test_large_msg_2M( iterations, timeout=0, verbose=False ):
    """
    Large message test - one sender, one receiver, 2 Mbyte messages.
    $ msgr-recv -a amqp://~0.0.0.0:9999 -c 100000 -R -b 100
    $ msgr-send -a amqp://0.0.0.0:9999 -c 100000 -b 2097152 -p 10 -R
    """

    msg_count = 1000
    port = free_tcp_ports()[0]

    receivers = []
    receiver = MessengerReceiverC()
    receiver.subscriptions = ["amqp://~0.0.0.0:%s" % port]
    receiver.receive_count = msg_count
    receiver.send_reply = False
    receiver.timeout = timeout
    receiver.recv_count = 100
    receivers.append( receiver )

    senders = []
    sender = MessengerSenderC()
    sender.targets = ["amqp://0.0.0.0:%s" % port]
    sender.send_count = msg_count
    sender.get_reply = False
    sender.send_batch = 2
    sender.msg_size = 2097152
    sender.timeout = timeout
    senders.append( sender )

    latencies = []
    throughputs = []

    for i in range(iterations):
        run_test( receivers, senders, verbose )
        results = parse_msgr_output( receiver.stdout() )
        latencies.append( results[0] )
        results = parse_msgr_output( sender.stdout() )
        throughputs.append( results[1] )

    return (latencies, throughputs)


def main(argv=None):
    """
    Run a series of benchmarks against Messenger.  Add the results to a database.
    """

    parser = optparse.OptionParser()
    parser.add_option("-l", "--label", action="store", default=time.asctime(),
                      help="label used to index this benchmark in the database [%default].")
    parser.add_option("-i", "--iterations", action="store", type="int", default=5,
                      help="# of times to repeat each test [%default].")
    parser.add_option("-v", "--verbose", action="store_true",
                      help="print extra detail to stdout.")
    parser.add_option("-t", "--timeout", action="store", type="int",
                      default=60,
                      help="set the default test timeout (in seconds) [%default].")
    parser.add_option("-d", "--db", action="store", default="./.msgr-db",
                      help="path to database for storing benchmark results [%default].")
    parser.add_option("--save", action="store_true", default=False,
                      help="store the results of the benchmark to the database [%default].")

    opts, extra = parser.parse_args(args=argv)

    if opts.save:
        if not os.path.exists( opts.db ):
            os.makedirs( opts.db );
        elif not os.path.isdir( opts.db ):
            raise TypeError("--db parameter must be a directory!")

    latency_filename = "%s/AL_%s.csv"
    throughput_filename = "%s/T_%s.csv"

    tests = [
        ("Loopback (64byte)", test_loopback_64),
        ("Large Msg (2Mbyte)", test_large_msg_2M)
        ]

    latencies = []
    throughputs = []

    for t in tests:
        if opts.verbose: print("Executing test '%s'..." % t[0])
        results = t[1](opts.iterations, opts.timeout, opts.verbose)
        if opts.verbose: print(" complete!")
        test_latencies = results[0]
        test_throughputs = results[1]

        if test_latencies:
            test_latencies.sort()
            low = test_latencies[0]
            high = test_latencies[-1]
            avg = sum(test_latencies)/len(test_latencies)
            latencies.append( (t[0], low, avg, high) )

        if test_throughputs:
            test_throughputs.sort()
            low = test_throughputs[0]
            high = test_throughputs[-1]
            avg = sum(test_throughputs)/len(test_throughputs)
            throughputs.append( (t[0], low, avg, high) )

    if opts.save:
        # gnuplot> set datafile separator ','
        # plot 'throughput.csv' using 0:3:2:4:xticlabels(1) with yerrorlines
        for l in latencies:
            # sanitize the test name so it may be used as a filename
            test_filename = re.sub('[)( ]', '_', l[0])
            with open(latency_filename % (opts.db, test_filename), 'ab') as f:
                writer = csv.writer(f)
                # (label, low, avg, hi)
                writer.writerows([(opts.label, l[1], l[2], l[3])])
                f.close()

        for t in throughputs:
            test_filename = re.sub('[)( ]', '_', l[0])
            with open(throughput_filename % (opts.db, test_filename), 'ab') as f:
                writer = csv.writer(f)
                # (label, low, avg, hi)
                writer.writerows([(opts.label, t[1], t[2], t[3])])
                f.close()

    header_format = "%-22s\t%16s\t%16s\t%16s"
    data_format =   "  %-20s\t%16.6f\t%16.6f\t%16.6f"
    if latencies:
        print(header_format % ("LATENCY (secs)", "min", "mean", "high"))
        for l in latencies:
            print(data_format % l)
    if throughputs:
        print(header_format % ("THROUGHPUT (msgs/sec)", "min", "mean", "high"))
        for t in throughputs:
            print(data_format % t )

    # The dreaded Ross-o-meter score:
    # normalize a "good" result to a score of 1000 (higher is better)
    #   latency of 1ms --> 1000, shorter values give higher score
    #   throughput of 100K msg/sec ---> 1000, greater rates give higher score
    print "\n"
    latency_rank = 0
    if latencies:
        l = [x[1] for x in latencies]
        avg = sum(l)/len(l)
        latency_rank = long(1.0/avg)
    print "Jross-o-meter latency rank = %d" % latency_rank

    throughput_rank = 0
    if throughputs:
        t = [x[1] for x in throughputs]
        avg = sum(t)/len(t)
        throughput_rank = long(avg/100)
    print "Jross-o-meter throughput rank = %d" % throughput_rank

    print "Combined Jross-o-meter ranking = %f" % ((latency_rank + throughput_rank)/2)
    return 0


if __name__ == "__main__":
    sys.exit(main())
